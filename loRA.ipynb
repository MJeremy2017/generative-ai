{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f67376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate==0.4.0 rouge_score==0.1.2 loralib==0.1.1 peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f18683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch, time\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcba3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e137386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model):\n",
    "    all_model_params = 0\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        cnt = param.numel()\n",
    "        all_model_params += cnt\n",
    "        if param.requires_grad:\n",
    "            trainable_params += cnt \n",
    "    print(f\"trainable params: {trainable_params}, % of trainable params {trainable_params*100/all_model_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1f341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 247577856, % of trainable params 100.00%\n"
     ]
    }
   ],
   "source": [
    "print_trainable_params(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c072fa",
   "metadata": {},
   "source": [
    "# Zero Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b6a10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "====================================================================================================\n",
      "\n",
      "Summarise the following conversation:\n",
      "---\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    \n",
      "Model inference:\n",
      "====================================================================================================\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "Human summary:\n",
      "====================================================================================================\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
     ]
    }
   ],
   "source": [
    "example_indices = [200]\n",
    "sep = \"=\" * 100\n",
    "for ind in example_indices:\n",
    "    text = dataset[\"test\"][\"dialogue\"][ind]\n",
    "    human_label = dataset[\"test\"][\"summary\"][ind]\n",
    "    tmp = f\"\"\"\n",
    "Summarise the following conversation:\n",
    "---\n",
    "{text}\n",
    "    \"\"\"\n",
    "    print(\"Input:\")\n",
    "    print(sep)\n",
    "    print(tmp)\n",
    "    inputs = tokenizer(tmp, return_tensors=\"pt\")\n",
    "    output = original_model.generate(**inputs)\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Model inference:\")\n",
    "    print(sep)\n",
    "    print(output_decoded)\n",
    "    \n",
    "    print(\"Human summary:\")\n",
    "    print(sep)\n",
    "    print(human_label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6df46",
   "metadata": {},
   "source": [
    "# Full Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6496a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(rows):\n",
    "    prompt_template = \"Summarise the following prompt:\\n{}\\nSummary:\"\n",
    "    texts = [prompt_template.format(d) for d in rows[\"dialogue\"]]\n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(rows[\"summary\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    # set -100 to padding tokens, which will be ignored by T5 during loss calculation\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "        for labels in targets[\"input_ids\"]\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f34966f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset.filter(lambda row, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b811cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8209c567ae174ac3a160ec37cfec1614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8da85cc9c043fab5af3044a7aed8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979e8a32aab04d6e97191a7c79e655b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY use 125 training examples\n",
    "tokenized_dataset = sub_dataset.map(tokenize_func, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba08fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"lora-summary-train-logs\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "570d3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOM on CPU\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1c274",
   "metadata": {},
   "source": [
    "## Human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdf3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = trainer.model.to(\"cpu\")\n",
    "_ = instruct_model.eval()\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "original_model == trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d675b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [1]\n",
    "sep = \"=\" * 100\n",
    "for ind in example_indices:\n",
    "    text = dataset[\"test\"][\"dialogue\"][ind]\n",
    "    human_label = dataset[\"test\"][\"summary\"][ind]\n",
    "    \n",
    "    prompt_template = f\"Summarise the following prompt:\\n{text}\\nSummary:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\")\n",
    "    output = instruct_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Model inference:\")\n",
    "    print(sep)\n",
    "    print(output_decoded)\n",
    "\n",
    "    output = original_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"\\nOriginal inference:\")\n",
    "    print(sep)\n",
    "    print(output_decoded)\n",
    "        \n",
    "    print(\"\\nHuman summary:\")\n",
    "    print(sep)\n",
    "    print(human_label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec9092",
   "metadata": {},
   "source": [
    "## Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "dialogues = dataset[\"test\"][\"dialogue\"][:N]\n",
    "summary = dataset[\"test\"][\"summary\"][:N]\n",
    "\n",
    "original_summaries = []\n",
    "instruct_summarise = []\n",
    "for i in tqdm(range(N)):\n",
    "    d = dialogues[i]\n",
    "    s = summary[i]\n",
    "    \n",
    "    prompt_template = f\"Summarise the following prompt:\\n{d}\\nSummary:\"\n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\")\n",
    "    \n",
    "    output = original_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    original_summaries.append(output_decoded)\n",
    "\n",
    "    output = instruct_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    instruct_summarise.append(output_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "original_model_score = rouge.compute(\n",
    "    predictions=original_summaries,\n",
    "    references=summary,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "instruct_model_score = rouge.compute(\n",
    "    predictions=instruct_summarise,\n",
    "    references=summary,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"Original model score:\\n\", original_model_score)\n",
    "print(\"Instruct model score:\\n\", instruct_model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa57f19",
   "metadata": {},
   "source": [
    "# loRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4034e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraModel, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b94f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3538944, % of trainable params 1.41%\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(\n",
    "    original_model, \n",
    "    config,\n",
    ")\n",
    "\n",
    "print_trainable_params(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ae367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset.filter(lambda row, index: index % 100 == 0, with_indices=True)\n",
    "tokenized_dataset = sub_dataset.map(tokenize_func, batched=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"lora-summary-train-logs\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-3,  # higher learning rate\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# a lot faster with lesser trainable weights\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc66c4b",
   "metadata": {},
   "source": [
    "## Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "dialogues = dataset[\"test\"][\"dialogue\"][:N]\n",
    "summary = dataset[\"test\"][\"summary\"][:N]\n",
    "\n",
    "original_summaries = []\n",
    "lora_summarise = []\n",
    "for i in tqdm(range(N)):\n",
    "    d = dialogues[i]\n",
    "    s = summary[i]\n",
    "    \n",
    "    prompt_template = f\"Summarise the following prompt:\\n{d}\\nSummary:\"\n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\")\n",
    "    \n",
    "    output = original_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    original_summaries.append(output_decoded)\n",
    "\n",
    "    output = lora_model.generate(\n",
    "        **inputs, \n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=200,\n",
    "            num_beams=1\n",
    "        )\n",
    "    )\n",
    "    output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    lora_summarise.append(output_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "original_model_score = rouge.compute(\n",
    "    predictions=original_summaries,\n",
    "    references=summary,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "lora_model_score = rouge.compute(\n",
    "    predictions=lora_summarise,\n",
    "    references=summary,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(\"Original model score:\\n\", original_model_score)\n",
    "print(\"PEFT loRA model score:\\n\", lora_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9688c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8d0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
